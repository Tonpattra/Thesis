## Summary of all the papers and interesting resources on creating numbers and letters in license plates using VAEs.
## Paper Reading
### 1. Graph Transformer Networks (NeurIPS, 2019)
|Problems| Limitation of existing Graph Neural Networks (GNNs) in handling heterogeneous graphs  |
|:------:|:-----|
|Related work| __1. Graph Neural Networks (GNNs)__ are machine learning models developed to perform various tasks on graphs. There are two approaches, spectral and non-spectral.<br> __2. Node classification with GNNs__ is used to predict a label for each node in a graph based on its structure and features  |
|Solution| Author porpose a new framework call Graph Transformer Networks(GTNs). The GTNs are designed to transform heterogeneous graphs into meta-path graphs, which are paths connecting different types of edges. The GTNs learn to generate these meta-path graphs and node representations |
|Result|Author compares the performance of different Graph Neural Network (GNN) models for node classification in heterogeneous graphs. The models compared are GCN, GAT, HAN, and the proposed GTN model with public 3 datasets are DBLP, ACM, IMDB. The results show that the GNN-based methods perform better than random walk-based network embedding methods.|
### 2. Comparing word2vec and GloVe for Automatic Measurement of MWE Compositionality (ACL, 2020)
|Problems|The authors aim to compare the effectiveness of two popular embedding methods, GloVe and word2vec, for automatic measurement of the semantic compositionality of multiword expressions task and to explore the suitability of different resources for compositionality assessment. The ultimate goal is to improve the automatic identification and processing of MWEs, which are important but challenging for natural language processing.|
|:------:|:-----|
|Related work| __1. Multiword expressions (MWEs)__ are phrases or expressions that contain more than one word and have a specific meaning that is not straightforward from the meanings of the specific words such as "Let the Cat Out of the Bag" (meaning reveal the secret). <br>__2. Semantic compositionality__ refers to how well the meaning of a phrase can be understood by looking at the meanings of the words it is made up of. If a phrase is compositional, like 'Picnic Basket', its meaning can be easily understood by knowing what 'picnic' and 'basket' mean. But if a phrase is non-compositional, like 'Spill the Beans', you need more context to understand its meaning because it's not clear just from looking at the individual words. |
|Solution| The authors select multiword expression candidates from two different corpora, sort them into batches, and evaluate them using word embeddings to measure their compositionality. They use two popular word embedding models, GloVe and word2vec, to compare their performance. |
|Result| The authors compare the performance of two word embedding models, GloVe and word2vec, for evaluating the compositionality of multiword expressions (MWEs). They found that the word2vec model had higher correlation with compositionality scores than the GloVe model, and thus, they preferred it for future work. |
### 3. RoBERTa: A Robustly Optimized BERT Pretraining Approach (ACL, 2019)
|Problems| The original BERT model faced certain challenges when dealing with large datasets, bigger batch sizes, and achieving top performance on various NLP tasks. Furthermore, the next sentence prediction (NSP) task used in BERT was found to be a less reliable signal for training the model.  |
|:------:|:-----|
|Related work| __1. BERT__ The original pre-trained language model introduced by Google in 2018, which uses a bidirectional transformer architecture to learn contextualized embeddings of words and sentences. <br> __2. ELMo__ A pre-trained language model that learns contextualized embeddings of words based on their surrounding context in a text corpus.
|Solution| The authors addressed some of the limitations of the original BERT model by removing the next sentence prediction (NSP) task, which was found to be a noisy training signal with limited usefulness. Additionally, they introduced a new training technique called "dynamic masking," which involves randomly masking each token in multiple ways during each epoch to encourage the model to learn more robust representations of the input text.
|Result| The authors introduced a new pre-training approach for language models that improved their performance on a range of NLP tasks compared to the original BERT model. The authors achieved state-of-the-art results on multiple benchmark datasets and demonstrated that their model could handle tasks requiring understanding of long-range dependencies in text.
### 4. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (ACL, 2018)
|Problems| The need for a more effective and efficient pre-training method that could capture bidirectional context in language understanding tasks.  |
|:------:|:-----|
|Related work| __1. Transformer__, the Transformer is a neural network architecture designed for sequence-to-sequence tasks such as machine translation. <br> __2. GPT__, the Generative Pre-trained Transformer (GPT) is a transformer-based language model that uses a left-to-right architecture and a language modeling pre-training objective.
|Solution| The authors introduce a new pre-training objective called Masked Language Modeling (MLM), which involves randomly masking some of the input tokens in a sequence and training the model to predict them based on the context of the other tokens.
|Result| The authors demonstrated that the BERT model outperformed existing models on 11 NLP tasks, including sentiment analysis, named entity recognition, and question answering.
